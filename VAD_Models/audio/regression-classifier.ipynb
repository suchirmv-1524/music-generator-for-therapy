{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11378681,"sourceType":"datasetVersion","datasetId":7122744},{"sourceId":11381615,"sourceType":"datasetVersion","datasetId":7126529}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport torchaudio\nfrom transformers import Wav2Vec2FeatureExtractor, HubertModel\nimport numpy as np\nimport math\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:31:34.984708Z","iopub.execute_input":"2025-04-12T18:31:34.985476Z","iopub.status.idle":"2025-04-12T18:31:34.989539Z","shell.execute_reply.started":"2025-04-12T18:31:34.985450Z","shell.execute_reply":"2025-04-12T18:31:34.988803Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class VADRegressor(nn.Module):\n    def __init__(self, input_dim=768, gru_stack_depth=1, hidden_dim=256):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=gru_stack_depth, batch_first=True, bidirectional=True)\n        self.attn_fc = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n        self.attn_softmax = nn.Softmax(dim=-1)\n        self.fc1 = nn.Linear(hidden_dim * 2, 256)\n        self.dropout = nn.Dropout(0.25)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 3)\n\n    def attention(self, x):\n        query = key = value = self.attn_fc(x)\n        d_k = query.size(-1)\n        attn_scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n        attn_weights = self.attn_softmax(attn_scores)\n        context = torch.matmul(attn_weights, value)\n        return context.sum(dim=1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        context = self.attention(gru_out)\n        x = torch.relu(self.fc1(context))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:31:37.524794Z","iopub.execute_input":"2025-04-12T18:31:37.525481Z","iopub.status.idle":"2025-04-12T18:31:37.532971Z","shell.execute_reply.started":"2025-04-12T18:31:37.525456Z","shell.execute_reply":"2025-04-12T18:31:37.532405Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class IEMOCAPDataset(Dataset):\n    def __init__(self, df, feature_extractor):\n        self.df = df\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio_path = row['filepath']\n        labels = torch.tensor([row['EmoVal'], row['EmoAct'], row['EmoDom']], dtype=torch.float32)\n\n        waveform, sample_rate = torchaudio.load(audio_path)\n        if sample_rate != 16000:\n            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n        waveform = waveform.squeeze(0)\n\n        inputs = self.feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n        return {'input_values': inputs['input_values'].squeeze(0)}, labels\n\ndef collate_fn(batch):\n    input_values = [item[0]['input_values'] for item in batch]\n    labels = torch.stack([item[1] for item in batch])\n    input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True)\n    return {'input_values': input_values}, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:31:41.284519Z","iopub.execute_input":"2025-04-12T18:31:41.284810Z","iopub.status.idle":"2025-04-12T18:31:41.290866Z","shell.execute_reply.started":"2025-04-12T18:31:41.284787Z","shell.execute_reply":"2025-04-12T18:31:41.290163Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"csv_path = \"/kaggle/input/iemocap/updated_iemocap_metadata.csv\"\ndf = pd.read_csv(csv_path)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\nhubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\").cuda()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:31:47.584619Z","iopub.execute_input":"2025-04-12T18:31:47.585179Z","iopub.status.idle":"2025-04-12T18:31:51.227272Z","shell.execute_reply.started":"2025-04-12T18:31:47.585155Z","shell.execute_reply":"2025-04-12T18:31:51.226680Z"}},"outputs":[{"name":"stdout","text":"Train: 8031, Val: 1004, Test: 1004\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd2cfba6c2a74d81a60a24ae85986c14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"808cb8599b1f4f8abd7403216df91e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46e24226cc284ea4b87410c6e8b8e209"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3e92519ccc47b78e62774af156a0b9"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_dataset = IEMOCAPDataset(train_df, feature_extractor)\nval_dataset = IEMOCAPDataset(val_df, feature_extractor)\ntest_dataset = IEMOCAPDataset(test_df, feature_extractor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n\nmodel = VADRegressor(input_dim=768).cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:32:30.971487Z","iopub.execute_input":"2025-04-12T18:32:30.971789Z","iopub.status.idle":"2025-04-12T18:32:31.069228Z","shell.execute_reply.started":"2025-04-12T18:32:30.971766Z","shell.execute_reply":"2025-04-12T18:32:31.068689Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for batch_inputs, labels in train_loader:\n        input_values = batch_inputs['input_values'].cuda()\n        labels = labels.cuda()\n\n        optimizer.zero_grad()\n        with torch.no_grad():\n            hubert_out = hubert_model(input_values).last_hidden_state\n\n        outputs = model(hubert_out)\n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        train_loss += loss.item()\n\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch_inputs, labels in val_loader:\n            input_values = batch_inputs['input_values'].cuda()\n            labels = labels.cuda()\n\n            hubert_out = hubert_model(input_values).last_hidden_state\n            outputs = model(hubert_out)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n    avg_train = train_loss / len(train_loader)\n    avg_val = val_loss / len(val_loader)\n    scheduler.step(avg_val)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train:.4f}, Val Loss: {avg_val:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:32:40.436228Z","iopub.execute_input":"2025-04-12T18:32:40.436500Z","iopub.status.idle":"2025-04-12T19:58:39.677365Z","shell.execute_reply.started":"2025-04-12T18:32:40.436478Z","shell.execute_reply":"2025-04-12T19:58:39.676702Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Train Loss: 4.8469, Val Loss: 1.0423\nEpoch 2/10, Train Loss: 21.8486, Val Loss: 1.2042\nEpoch 3/10, Train Loss: 3.3898, Val Loss: 1.2999\nEpoch 4/10, Train Loss: 1.7979, Val Loss: 0.5945\nEpoch 5/10, Train Loss: 2.2594, Val Loss: 0.7895\nEpoch 6/10, Train Loss: 1.0268, Val Loss: 0.6343\nEpoch 7/10, Train Loss: 3.4467, Val Loss: 0.7481\nEpoch 8/10, Train Loss: 0.5916, Val Loss: 0.5448\nEpoch 9/10, Train Loss: 0.8344, Val Loss: 0.6399\nEpoch 10/10, Train Loss: 124.2276, Val Loss: 0.5714\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model.eval()\ntest_loss = 0.0\nwith torch.no_grad():\n    for batch_inputs, labels in test_loader:\n        input_values = batch_inputs['input_values'].cuda()\n        labels = labels.cuda()\n\n        hubert_out = hubert_model(input_values).last_hidden_state\n        outputs = model(hubert_out)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n\nprint(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:01:30.349140Z","iopub.execute_input":"2025-04-12T20:01:30.349430Z","iopub.status.idle":"2025-04-12T20:02:31.595812Z","shell.execute_reply.started":"2025-04-12T20:01:30.349407Z","shell.execute_reply":"2025-04-12T20:02:31.595026Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 0.6045\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.save(model.state_dict(), \"vad_regressor.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:04:27.159058Z","iopub.execute_input":"2025-04-12T20:04:27.159346Z","iopub.status.idle":"2025-04-12T20:04:27.176528Z","shell.execute_reply.started":"2025-04-12T20:04:27.159325Z","shell.execute_reply":"2025-04-12T20:04:27.176016Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom transformers import Wav2Vec2FeatureExtractor, HubertModel\nimport torchaudio\n\n# Load the saved model weights\nmodel = VADRegressor(input_dim=768)  # Use the same model architecture you trained\nmodel.load_state_dict(torch.load(\"/kaggle/working/vad_regressor.pth\", map_location=torch.device('cpu')))  # Load the trained weights onto CPU\nmodel = model.cpu()  # Ensure model is on CPU\nmodel.eval()  # Set the model to evaluation mode\n\n# Load the feature extractor (Wav2Vec2 or HuBERT, depending on the original model)\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n\n# Function to predict VAD scores from an audio file\ndef predict_vad(audio_path):\n    # Load and preprocess audio\n    waveform, sample_rate = torchaudio.load(audio_path)\n    if sample_rate != 16000:\n        waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n    waveform = waveform.squeeze(0)  # Remove channel dim if mono\n\n    # Extract features\n    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.squeeze(0) for k, v in inputs.items()}  # Remove batch dim\n\n    # Predict VAD scores\n    with torch.no_grad():\n        hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n        hubert_outputs = hubert_model(input_values=inputs['input_values'].unsqueeze(0)).last_hidden_state  # (1, seq_len, 768)\n        vad_scores = model(hubert_outputs)\n\n    return vad_scores.cpu().numpy()\n\n# Example usage:\naudio_path = \"/kaggle/input/iemocap/iemocap_audio/Ses01F_impro01_F000.wav\"\nvad_scores = predict_vad(audio_path)\nprint(\"Predicted VAD scores:\", vad_scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:28:12.705861Z","iopub.execute_input":"2025-04-12T20:28:12.706132Z","iopub.status.idle":"2025-04-12T20:28:13.364058Z","shell.execute_reply.started":"2025-04-12T20:28:12.706111Z","shell.execute_reply":"2025-04-12T20:28:13.363296Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/171980140.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/vad_regressor.pth\", map_location=torch.device('cpu')))  # Load the trained weights onto CPU\n","output_type":"stream"},{"name":"stdout","text":"Predicted VAD scores: [[2.3477838 2.1251054 2.2284825]]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch\nfrom transformers import Wav2Vec2FeatureExtractor, HubertModel\nimport torchaudio\n\n# Load the saved model weights\nmodel = VADRegressor(input_dim=768)  # Use the same model architecture you trained\nmodel.load_state_dict(torch.load(\"/kaggle/working/vad_regressor.pth\", map_location=torch.device('cpu')))  # Load the trained weights onto CPU\nmodel = model.cpu()  # Ensure model is on CPU\nmodel.eval()  # Set the model to evaluation mode\n\n# Load the feature extractor (Wav2Vec2 or HuBERT, depending on the original model)\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n\n# Function to predict VAD scores from an audio file\ndef predict_vad(audio_path):\n    # Load and preprocess audio\n    waveform, sample_rate = torchaudio.load(audio_path)\n    \n    # Resample to 16kHz if necessary\n    if sample_rate != 16000:\n        waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n    \n    # Convert to mono (if stereo or multi-channel)\n    if waveform.shape[0] > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)  # Averaging the channels\n    \n    waveform = waveform.squeeze(0)  # Remove channel dim if mono\n\n    # Extract features using the feature extractor\n    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.squeeze(0) for k, v in inputs.items()}  # Remove batch dim\n\n    # Predict VAD scores\n    with torch.no_grad():\n        # Load the pre-trained HuBERT model to extract features\n        hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n        \n        # Extract features from HuBERT\n        hubert_outputs = hubert_model(input_values=inputs['input_values'].unsqueeze(0)).last_hidden_state  # (1, seq_len, 768)\n        \n        # Predict the VAD scores using your trained model\n        vad_scores = model(hubert_outputs)\n\n    return vad_scores.cpu().numpy()\n\n# Example usage with custom recorded audio:\naudio_path = \"/kaggle/input/iemocap/iemocap_audio/Ses01F_impro01_F000.wav\"  # Path to your custom uploaded audio file\nvad_scores = predict_vad(audio_path)\nprint(\"Predicted VAD scores:\", vad_scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:29:36.805314Z","iopub.execute_input":"2025-04-12T20:29:36.805597Z","iopub.status.idle":"2025-04-12T20:29:37.371432Z","shell.execute_reply.started":"2025-04-12T20:29:36.805575Z","shell.execute_reply":"2025-04-12T20:29:37.370796Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3545680707.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/vad_regressor.pth\", map_location=torch.device('cpu')))  # Load the trained weights onto CPU\n","output_type":"stream"},{"name":"stdout","text":"Predicted VAD scores: [[2.3477838 2.1251054 2.2284825]]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}